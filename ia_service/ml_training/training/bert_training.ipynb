{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0533841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9318a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a2e9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../news_datasets/full_training_dataset.csv\"\n",
    "BERT_OUTPUT_DIR = 'classification_report/distilbert_classification_model'\n",
    "encoder_filename_pkl = 'classification_report/bert_label_encoder.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c749ce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with shape: (398, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file)\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c9821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X = df['Full Text']\n",
    "y = df['llm_category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder().fit(y_train)\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "NUM_LABELS = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "230153ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Class weights calculated (inversely proportional to frequency) :\n",
      "tensor([0.4229, 0.5372, 1.2823, 3.0577, 1.4722, 1.0743, 1.7283, 2.0921],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#  Calculate class weights to handle class imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_encoded),\n",
    "    y=y_train_encoded\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "print(\"\\n[INFO] Class weights calculated (inversely proportional to frequency) :\")\n",
    "print(class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a07a5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tokenization\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "MAX_LENGTH = 512 # Maximum length for DistilBERT\n",
    "\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(X_train)\n",
    "test_encodings = tokenize_data(X_test)\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# Create Datasets and Dataloaders class\n",
    "class EnvironmentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EnvironmentDataset(train_encodings, y_train_tensor)\n",
    "test_dataset = EnvironmentDataset(test_encodings, y_test_tensor)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_dataoader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b448d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class modified to ponderate loss with class weights\n",
    "class WeightedDistilBert(nn.Module):\n",
    "    def __init__(self, num_labels, class_weights):\n",
    "        super(WeightedDistilBert, self).__init__()\n",
    "        self.distilbert = DistilBertForSequenceClassification.from_pretrained(\n",
    "            'distilbert-base-uncased',\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss=None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits.view(-1, self.distilbert.num_labels), labels.view(-1))   \n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b904f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 / 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:11<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 2 / 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:09<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 3 / 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:09<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "model = WeightedDistilBert(NUM_LABELS, class_weights_tensor).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5) # Typical learning rate for fine-tuning\n",
    "\n",
    "# Training Loop\n",
    "def train_bert(model, dataloader, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n--- Epoch {epoch + 1} / {epochs} ---\")\n",
    "        for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            loss, _ = model(input_ids, attention_mask, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "train_bert(model, train_dataoader, optimizer, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ed59ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  9.90it/s]\n",
      "/var/www/html/ecoSynthesIA/ia_service/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Accuracy for model BERT : 80 datas : 0.7125\n",
      "\n",
      "Classification report (BERT) :\n",
      "                                     precision    recall  f1-score   support\n",
      "\n",
      "        BIODIVERSITY AND ECOSYSTEMS       0.86      1.00      0.92        24\n",
      "              CLIMATE AND EMISSIONS       0.77      0.53      0.62        19\n",
      "              ENERGY AND TRANSITION       0.60      0.75      0.67         8\n",
      "                  NATURAL RESOURCES       0.00      0.00      0.00         3\n",
      "            POLICIES AND REGULATION       0.60      0.43      0.50         7\n",
      "POLLUTION AND ENVIRONMENTAL QUALITY       0.60      1.00      0.75         9\n",
      "                RISKS AND DISASTERS       0.62      1.00      0.77         5\n",
      "              SOCIO-ECONOMIC IMPACT       0.00      0.00      0.00         5\n",
      "\n",
      "                           accuracy                           0.71        80\n",
      "                          macro avg       0.51      0.59      0.53        80\n",
      "                       weighted avg       0.66      0.71      0.67        80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/www/html/ecoSynthesIA/ia_service/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/var/www/html/ecoSynthesIA/ia_service/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "def evaluate_bert(model, dataloader, encoder):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            _, logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            all_preds.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    target_names = encoder.classes_\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nðŸš€ Accuracy for model BERT : {len(all_labels)} datas : {accuracy:.4f}\")\n",
    "    \n",
    "    report = classification_report(all_labels, all_preds, target_names=target_names)\n",
    "    print(\"\\nClassification report (BERT) :\")\n",
    "    print(report)\n",
    "    \n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Lancement de l'Ã©valuation\n",
    "all_labels, all_preds = evaluate_bert(model, test_dataloader, label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9f21512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DistilBERT model and Tokenizer registered in : classification_report/distilbert_classification_model\n",
      "âœ… Label Encoder registered in : classification_report/bert_label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model and tokenizer\n",
    "os.makedirs(BERT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "model.distilbert.save_pretrained(BERT_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(BERT_OUTPUT_DIR)\n",
    "\n",
    "with open(encoder_filename_pkl, 'wb') as encoder_file:\n",
    "    pickle.dump(label_encoder, encoder_file)\n",
    "\n",
    "print(f\"âœ… DistilBERT model and Tokenizer registered in : {BERT_OUTPUT_DIR}\")\n",
    "print(f\"âœ… Label Encoder registered in : {encoder_filename_pkl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb71350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
